{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1tmMYr3iCX5JU_6fnmvVEHLJnow_8hkGH","timestamp":1731009621903}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"0W6ZKGvdvsSL"},"outputs":[],"source":["# ***** PYTORCH DOCUMENTATION - EASE TO IMPLEMENTATION, FIND METHODS/FUNCTIONS SUITABLE FOR TASK ******\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Loading data from GitHub because I think it's easier\n","url_train = 'https://raw.githubusercontent.com/ElsieElsevier/BIAI/refs/heads/main/training_data.csv'\n","url_test = 'https://raw.githubusercontent.com/ElsieElsevier/BIAI/refs/heads/main/test_data.csv'\n","\n","df_train = pd.read_csv(url_train)\n","df_test = pd.read_csv(url_test)"]},{"cell_type":"code","source":["# DATA EXPLORATION\n","stars = '*************************************************'\n","\n","# Print the columns of the training and test datasets\n","print(f\"Train dataset columns: {df_train.columns.tolist()}\")\n","print(f\"Test dataset columns: {df_test.columns.tolist()}\")\n","print(stars)\n","\n","# Information about the training dataset\n","print(\"Training Data Info:\")\n","df_train.info()\n","print(stars)\n","\n","# Information about the test dataset\n","print(\"Test Data Info:\")\n","df_test.info()\n","print(stars)\n","\n","# Check for missing values in the training dataset\n","print(\"Missing values in train dataset:\")\n","print(df_train.isnull().sum())\n","print(stars)\n","\n","# Check for missing values in the test dataset\n","print(\"Missing values in test dataset:\")\n","print(df_test.isnull().sum())\n","print(stars)\n","\n","# Check unique values in each column for the training dataset\n","print(\"Unique values in train dataset:\")\n","for column in df_train.columns:\n","    unique_count = df_train[column].nunique()\n","    print(f\"{column}: {unique_count} unique values\")\n","print(stars)\n","\n","# Check unique values in each column for the test dataset\n","print(\"Unique values in test dataset:\")\n","for column in df_test.columns:\n","    unique_count = df_test[column].nunique()\n","    print(f\"{column}: {unique_count} unique values\")\n","print(stars)\n","\n","# Display unique values for specific columns in the training dataset\n","print(f\"Unique values in 'workclass' column of train dataset: {df_train['workclass'].unique()}\")\n","print(stars)\n","print(f\"Unique values in 'occupation' column of train dataset: {df_train['occupation'].unique()}\")\n","print(stars)\n","print(f\"Unique values in 'relationship' column of train dataset: {df_train['relationship'].unique()}\")\n","print(stars)\n","print(f\"Unique values in 'native_country' column of train dataset: {df_train['native_country'].unique()}\")\n","print(stars)\n","print(f\"Unique values in 'marital_status' column of train dataset: {df_train['marital_status'].unique()}\")\n","print(stars)\n","print(f\"Unique values in 'income_bracket' column of train dataset: {df_train['income_bracket'].unique()}\")\n","print(stars)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"POIzOAlT35-d","outputId":"5b6fe1df-8334-4272-a864-9272a673d971","executionInfo":{"status":"ok","timestamp":1731098150040,"user_tz":360,"elapsed":358,"user":{"displayName":"Jin Bai","userId":"02958359284509703486"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train dataset columns: ['Unnamed: 0', 'age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship', 'race', 'gender', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income_bracket']\n","Test dataset columns: ['Unnamed: 0', 'age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship', 'race', 'gender', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income_bracket']\n","*************************************************\n","Training Data Info:\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 32561 entries, 0 to 32560\n","Data columns (total 16 columns):\n"," #   Column          Non-Null Count  Dtype \n","---  ------          --------------  ----- \n"," 0   Unnamed: 0      32561 non-null  int64 \n"," 1   age             32561 non-null  int64 \n"," 2   workclass       32561 non-null  object\n"," 3   fnlwgt          32561 non-null  int64 \n"," 4   education       32561 non-null  object\n"," 5   education_num   32561 non-null  int64 \n"," 6   marital_status  32561 non-null  object\n"," 7   occupation      32561 non-null  object\n"," 8   relationship    32561 non-null  object\n"," 9   race            32561 non-null  object\n"," 10  gender          32561 non-null  object\n"," 11  capital_gain    32561 non-null  int64 \n"," 12  capital_loss    32561 non-null  int64 \n"," 13  hours_per_week  32561 non-null  int64 \n"," 14  native_country  32561 non-null  object\n"," 15  income_bracket  32561 non-null  object\n","dtypes: int64(7), object(9)\n","memory usage: 4.0+ MB\n","*************************************************\n","Test Data Info:\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 16282 entries, 0 to 16281\n","Data columns (total 16 columns):\n"," #   Column          Non-Null Count  Dtype  \n","---  ------          --------------  -----  \n"," 0   Unnamed: 0      16282 non-null  int64  \n"," 1   age             16282 non-null  object \n"," 2   workclass       16281 non-null  object \n"," 3   fnlwgt          16281 non-null  float64\n"," 4   education       16281 non-null  object \n"," 5   education_num   16281 non-null  float64\n"," 6   marital_status  16281 non-null  object \n"," 7   occupation      16281 non-null  object \n"," 8   relationship    16281 non-null  object \n"," 9   race            16281 non-null  object \n"," 10  gender          16281 non-null  object \n"," 11  capital_gain    16281 non-null  float64\n"," 12  capital_loss    16281 non-null  float64\n"," 13  hours_per_week  16281 non-null  float64\n"," 14  native_country  16281 non-null  object \n"," 15  income_bracket  16281 non-null  object \n","dtypes: float64(5), int64(1), object(10)\n","memory usage: 2.0+ MB\n","*************************************************\n","Missing values in train dataset:\n","Unnamed: 0        0\n","age               0\n","workclass         0\n","fnlwgt            0\n","education         0\n","education_num     0\n","marital_status    0\n","occupation        0\n","relationship      0\n","race              0\n","gender            0\n","capital_gain      0\n","capital_loss      0\n","hours_per_week    0\n","native_country    0\n","income_bracket    0\n","dtype: int64\n","*************************************************\n","Missing values in test dataset:\n","Unnamed: 0        0\n","age               0\n","workclass         1\n","fnlwgt            1\n","education         1\n","education_num     1\n","marital_status    1\n","occupation        1\n","relationship      1\n","race              1\n","gender            1\n","capital_gain      1\n","capital_loss      1\n","hours_per_week    1\n","native_country    1\n","income_bracket    1\n","dtype: int64\n","*************************************************\n","Unique values in train dataset:\n","Unnamed: 0: 32561 unique values\n","age: 73 unique values\n","workclass: 9 unique values\n","fnlwgt: 21648 unique values\n","education: 16 unique values\n","education_num: 16 unique values\n","marital_status: 7 unique values\n","occupation: 15 unique values\n","relationship: 6 unique values\n","race: 5 unique values\n","gender: 2 unique values\n","capital_gain: 119 unique values\n","capital_loss: 92 unique values\n","hours_per_week: 94 unique values\n","native_country: 42 unique values\n","income_bracket: 2 unique values\n","*************************************************\n","Unique values in test dataset:\n","Unnamed: 0: 16282 unique values\n","age: 74 unique values\n","workclass: 9 unique values\n","fnlwgt: 12787 unique values\n","education: 16 unique values\n","education_num: 16 unique values\n","marital_status: 7 unique values\n","occupation: 15 unique values\n","relationship: 6 unique values\n","race: 5 unique values\n","gender: 2 unique values\n","capital_gain: 113 unique values\n","capital_loss: 82 unique values\n","hours_per_week: 89 unique values\n","native_country: 41 unique values\n","income_bracket: 2 unique values\n","*************************************************\n","Unique values in 'workclass' column of train dataset: [' State-gov' ' Self-emp-not-inc' ' Private' ' Federal-gov' ' Local-gov'\n"," ' ?' ' Self-emp-inc' ' Without-pay' ' Never-worked']\n","*************************************************\n","Unique values in 'occupation' column of train dataset: [' Adm-clerical' ' Exec-managerial' ' Handlers-cleaners' ' Prof-specialty'\n"," ' Other-service' ' Sales' ' Craft-repair' ' Transport-moving'\n"," ' Farming-fishing' ' Machine-op-inspct' ' Tech-support' ' ?'\n"," ' Protective-serv' ' Armed-Forces' ' Priv-house-serv']\n","*************************************************\n","Unique values in 'relationship' column of train dataset: [' Not-in-family' ' Husband' ' Wife' ' Own-child' ' Unmarried'\n"," ' Other-relative']\n","*************************************************\n","Unique values in 'native_country' column of train dataset: [' United-States' ' Cuba' ' Jamaica' ' India' ' ?' ' Mexico' ' South'\n"," ' Puerto-Rico' ' Honduras' ' England' ' Canada' ' Germany' ' Iran'\n"," ' Philippines' ' Italy' ' Poland' ' Columbia' ' Cambodia' ' Thailand'\n"," ' Ecuador' ' Laos' ' Taiwan' ' Haiti' ' Portugal' ' Dominican-Republic'\n"," ' El-Salvador' ' France' ' Guatemala' ' China' ' Japan' ' Yugoslavia'\n"," ' Peru' ' Outlying-US(Guam-USVI-etc)' ' Scotland' ' Trinadad&Tobago'\n"," ' Greece' ' Nicaragua' ' Vietnam' ' Hong' ' Ireland' ' Hungary'\n"," ' Holand-Netherlands']\n","*************************************************\n","Unique values in 'marital_status' column of train dataset: [' Never-married' ' Married-civ-spouse' ' Divorced'\n"," ' Married-spouse-absent' ' Separated' ' Married-AF-spouse' ' Widowed']\n","*************************************************\n","Unique values in 'income_bracket' column of train dataset: [' <=50K' ' >50K']\n","*************************************************\n"]}]},{"cell_type":"code","source":["# DATA PRE-PROCESSING PART 1\n","\n","# Listing my categorical and numeric features for readability and reference\n","categorical_features = [\n","    'workclass',\n","    'marital_status',\n","    'occupation',\n","    'relationship',\n","    'race', 'gender',\n","    'native_country'\n","    ]\n","\n","numerical_features = [\n","    'age',\n","    #'fnlwgt', # using fnlwgt actually hurts our predictions\n","    'education_num',\n","    'capital_gain',\n","    'capital_loss',\n","    'hours_per_week'\n","    ]\n","\n","def clean_train_data(df_train): # OKAY\n","    \"\"\"Cleaning training data\"\"\"\n","    # 1. Drops first column (index column), education, and fnlwgt\n","    df_train = df_train.drop(df_train.columns[0], axis=1)\n","    df_train = df_train.drop('education', axis=1)\n","    df_train = df_train.drop('fnlwgt', axis=1) # Re-implement if needed for training\n","    return df_train\n","\n","def clean_test_data(df_test): # OKAY\n","    \"\"\"Cleaning test data\"\"\"\n","    # 1. Remove row with '|1x3 Cross validator' string in the 'age' feature\n","    df_test = df_test[~df_test['age'].astype(str).str.contains('Cross validator')]\n","\n","    # 2. Drops first column (index column), education, and fnlwgy\n","    df_test = df_test.drop(df_test.columns[0], axis=1)\n","    df_test = df_test.drop('education', axis=1)\n","    df_test = df_test.drop('fnlwgt', axis=1) # Re-implement if needed for training\n","    return df_test\n","\n","def process_features(df, scaler=None, train_columns=None):\n","  # Need scaler and train_colum param to differentiate between train/test\n","    \"\"\"Feature processing\"\"\"\n","    # 1. Remove whitespace and replace '?' with NaN in categorical_features list\n","    for feature in categorical_features:\n","        df[feature] = df[feature].str.strip()\n","        df[feature] = df[feature].replace('?', np.nan)\n","\n","    # 2. Clean income_bracket label by removing whitespaces and periods\n","    df['income_bracket'] = df['income_bracket'].str.strip()\n","    df['income_bracket'] = df['income_bracket'].str.replace('.', '')\n","\n","    # 3. Create dictionary for mapping label\n","      # I have to specify the extra space for some reason because tensor error\n","    label_map = {\n","        ' <=50K': 0,\n","        '<=50K': 0,\n","        ' >50K': 1,\n","        '>50K': 1\n","    }\n","    df['income_bracket'] = df['income_bracket'].map(label_map).fillna(0) # Map to dic\n","\n","    # 3. Apply imputation to missing values\n","      # Use mode imputation for categorical and mean imputation for numeric\n","    categorical_imputer = SimpleImputer(strategy='most_frequent')\n","    numerical_imputer = SimpleImputer(strategy='mean')\n","    df[categorical_features] = categorical_imputer.fit_transform(df[categorical_features]) # Apply imputation\n","    df[numerical_features] = numerical_imputer.fit_transform(df[numerical_features]) # Apply imputation\n","\n","    # 4. Scaling numeric features\n","    if scaler is None: # Create/fit scaler for new training data\n","        scaler = StandardScaler()\n","        df[numerical_features] = scaler.fit_transform(df[numerical_features])\n","    else: # If scaler is present, transform the data\n","          # Only applicable to test data\n","        df[numerical_features] = scaler.transform(df[numerical_features])\n","\n","    # 6. Encode gender using LabelEncoder\n","    le_gender = LabelEncoder()\n","    df['gender'] = le_gender.fit_transform(df['gender'])\n","\n","    # 7. One-hot encode other categorical features (drop_first=True to avoid multicollinearity)\n","    onehot_features = [\n","        'workclass',\n","        'marital_status',\n","        'occupation',\n","        'relationship',\n","        'race',\n","        'native_country'\n","        ]\n","\n","      # Apply one-hot encoding\n","      # Got this one-hot encoding tweak from Claude-Sonnet 3.5\n","    if train_columns is None:  # Denote this is training data\n","        df_encoded = pd.get_dummies(df[onehot_features], prefix=onehot_features, drop_first=True)\n","        train_columns = df_encoded.columns\n","\n","        # Drop original categorical columns and add encoded ones\n","        df = df.drop(columns=onehot_features)\n","        df = pd.concat([df, df_encoded], axis=1)\n","\n","        return df, scaler, train_columns\n","\n","    else:  # if train_column is not none then it denote test data\n","        df_encoded = pd.get_dummies(df[onehot_features], prefix=onehot_features)\n","\n","        # Makes sure test and train data have the same number of columns\n","        for col in train_columns:\n","            if col not in df_encoded.columns:\n","                df_encoded[col] = 0\n","        df_encoded = df_encoded[train_columns] # Reorder columns to match\n","\n","        # Drop original categorical columns and add encoded ones\n","        df = df.drop(columns=onehot_features)\n","        df = pd.concat([df, df_encoded], axis=1)\n","\n","        return df, scaler"],"metadata":{"id":"weWqIl-14OM6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# DATA PRE-PROCESSING PART 2\n","\n","def neural_network_prep(df_train, df_test): # OKAY\n","    \"\"\"Prepare both datasets for neural network training\"\"\"\n","    # 1. Clean both datasets using function\n","    df_train = clean_train_data(df_train.copy())\n","    df_test = clean_test_data(df_test.copy())\n","\n","    # 2. Process train features - DO NOT USE UNPACKING METHOD, USE INDEX METHOD\n","    result_train = process_features(df_train)  # Get training results\n","    df_train_processed = result_train[0]  # Get processed dataframe\n","    scaler = result_train[1]  # Get scaler\n","    train_columns = result_train[2]\n","\n","    # Process test features\n","    result_test = process_features(df_test, scaler=scaler, train_columns=train_columns)  # Denotes that it's the test data\n","    df_test_processed = result_test[0]  # Get processed dataframe\n","\n","    # 3. Separate features and labels\n","      # Don't use scikit-learn because we already have separated train/test datasets - Claude-Sonnect 3.5\n","    X_train = df_train_processed.drop('income_bracket', axis=1).values\n","    y_train = df_train_processed['income_bracket'].values\n","    X_test = df_test_processed.drop('income_bracket', axis=1).values\n","    y_test = df_test_processed['income_bracket'].values\n","\n","    # 4. Convert to float32 for efficiency/speed\n","    X_train = X_train.astype(np.float32)\n","    X_test = X_test.astype(np.float32)\n","    y_train = y_train.astype(np.float32)\n","    y_test = y_test.astype(np.float32)\n","\n","    # 5. Using verify_data to confirm data is ready for ingestion\n","    verify_data(X_train, y_train, \"Training\")\n","    verify_data(X_test, y_test, \"Test\")\n","\n","    return X_train, y_train, X_test, y_test\n","\n","\n","def verify_data(X, y, name): # OKAY - FOR VERIFICATION THAT DATA IS NEURAL NETWORK READY\n","    \"\"\"Verify dataset is ready for neural network\"\"\"\n","    print(f\"\\n{name} Dataset Verification:\")\n","    print(f\"Feature shape: {X.shape}\")\n","    print(f\"Label shape: {y.shape}\")\n","    print(f\"Feature dtype: {X.dtype}\")\n","    print(f\"Label dtype: {y.dtype}\")\n","    print(f\"Any NaN in features: {np.isnan(X).any()}\")\n","    print(f\"Any NaN in labels: {np.isnan(y).any()}\")\n","    print(f\"Feature value range: [{X.min():.3f}, {X.max():.3f}]\")\n","    print(f\"Unique label values: {np.unique(y)}\")"],"metadata":{"id":"Ln4dhw0_xSHX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# DATA MODELING USING PYTORCH\n","\n","class IncomeDataset(Dataset):\n","    \"\"\"Create custom dataset for income prediction\"\"\"\n","    def __init__(self, X, y): # Need to initialize constructor, X and y param for inputs\n","\n","        self.X = torch.FloatTensor(X) # Convert to PyTorch tensors\n","        # Need to ensure y is a float that's between 0,1 and reshape it to the expected shape\n","        # Tensor expects this specific output or else won't run - Claude-Sonnect 3.5\n","        self.y = torch.FloatTensor(y.astype(float)).view(-1, 1) # Convert to PyTorch tensors\n","\n","    # Creating custom dataset - Claude-Sonnect 3.5\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","\n","# 2. Create a neural network class\n","class IncomeClassifier(nn.Module):\n","    \"\"\"Create neural network class and its layers\"\"\"\n","    def __init__(self, input_size): # Need to initialize, use input_size parameter for first hidden layer\n","        super(IncomeClassifier, self).__init__() # Not sure what this does but need it to run - Claude-Sonnect 3.5\n","\n","        # Neural Network Topography\n","        self.model = nn.Sequential(\n","\n","            # First layer\n","            nn.Linear(input_size, 128),\n","            nn.BatchNorm1d(128),\n","            nn.ReLU(),\n","            nn.Dropout(0.4),\n","\n","            # Second layer\n","            nn.Linear(128, 64),\n","            nn.BatchNorm1d(64),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","\n","            # Third layer\n","            nn.Linear(64, 32),\n","            nn.BatchNorm1d(32),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","\n","            # Output layer\n","            nn.Linear(32, 1),\n","            nn.Sigmoid() # Sigmoid because binary classification\n","        )\n","\n","    def forward(self, x): # Defines forward movement by passing in \"X\" to every layer\n","        return self.model(x)"],"metadata":{"id":"HIV-9zI3w09j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ***** PYTORCH DOCUMENTATION - EASE TO IMPLEMENTATION, FIND METHODS/FUNCTIONS SUITABLE FOR TASK ******\n","\n","def train_model(model, train_loader, val_loader, criterion, optimizer, max_epochs=50, patience=10, min_delta=1e-5):\n","    \"\"\"Creating the training loop with early stoppage\"\"\"\n","  # model: PyTorch model\n","  # train_loader: Training data loader\n","  # val_loader: Validation data loader\n","  # criterion: Loss function\n","  # optimizer: Optimizer (Adam)\n","  # max_epochs: Maximum number of epochs to train = 50\n","  # patience: Number of epochs to wait for improvement before stopping = 10\n","  # min_delta: Minimum change in validation loss to qualify as an improvement\n","\n","    # Initialize variables\n","    best_val_loss = float('inf') # Initialize to infinity to keep track of best scores - Claude-Sonnet 3.5\n","    patience_counter = 0\n","    best_model_state = None\n","    train_losses = []\n","    val_losses = []\n","\n","    for epoch in range(max_epochs): # Start iterating over my epochs\n","        model.train() # Training phase\n","        total_loss = 0\n","        correct = 0\n","        total = 0\n","\n","        for inputs, labels in train_loader: # Iterates over training loader data\n","            outputs = model(inputs) # Forward propagation\n","            loss = criterion(outputs, labels)\n","            optimizer.zero_grad() # Backward propagation\n","            loss.backward() # Calculate gradient\n","            optimizer.step() # Update model based on gradient\n","\n","            # Accuracy Score\n","            predicted = (outputs > 0.5).float()\n","            total += labels.size(0) # Counting total labels\n","            correct += (predicted == labels).sum().item() # Original sum didn't work - Claude-Sonnet 3.5\n","            total_loss += loss.item() # Counting losses\n","\n","        # Training metrics - Claude-Sonnect 3.5\n","        epoch_loss = total_loss / len(train_loader)\n","        epoch_acc = 100 * correct / total\n","        train_losses.append(epoch_loss)\n","\n","        # Validation\n","        model.eval() # Must set model to eval mode\n","        val_loss = 0\n","        val_correct = 0\n","        val_total = 0\n","\n","        with torch.no_grad(): # No need to save gradient since this is validation using test dataset\n","            for inputs, labels in val_loader:  # Iterates over the test data in val_loader\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","                val_loss += loss.item()\n","\n","                predicted = (outputs > 0.5).float()\n","                val_total += labels.size(0)\n","                val_correct += (predicted == labels).sum().item()\n","\n","        # Metrics for validation phase\n","        val_epoch_loss = val_loss / len(val_loader)\n","        val_epoch_acc = 100 * val_correct / val_total\n","        val_losses.append(val_epoch_loss)\n","\n","        # Print epoch statistics\n","        print(f'Epoch [{epoch+1}/{max_epochs}]: '\n","              f'Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}%, '\n","              f'Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.2f}%')\n","\n","        # Early stoppage\n","        if val_epoch_loss < (best_val_loss - min_delta): # If current validation is better, start counting from 0 again\n","            best_val_loss = val_epoch_loss\n","            patience_counter = 0\n","            best_model_state = model.state_dict().copy()\n","        else: # If current validation is worst, add 1 count until you hit the patience count and stop the loop\n","            patience_counter += 1\n","            if patience_counter >= patience:\n","                print(f'\\nEarly stopping triggered after {epoch+1} epochs')\n","                model.load_state_dict(best_model_state) # Restore the best model\n","\n","                return train_losses, val_losses\n","\n","    return train_losses, val_losses\n","\n"],"metadata":{"id":"_yMi2sdL69Ym"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def main():\n","    '''Creating the main function with validation split'''\n","    # Label data - USE INDEX BECAUSE WE DON'T USE UNPACKING\n","    result = neural_network_prep(df_train, df_test)\n","    X_train = result[0]\n","    y_train = result[1]\n","    X_test = result[2]\n","    y_test = result[3]\n","\n","    # Split training data into train and validation sets (80-20 split)\n","    # Uses validation from train_data to tune\n","    # Not using scikit-learn because it gets messy imo with splitting training data with a real test dataset, etc\n","    train_size = int(0.8 * len(X_train))\n","    X_train_split = X_train[:train_size]\n","    y_train_split = y_train[:train_size]\n","    X_val = X_train[train_size:]\n","    y_val = y_train[train_size:]\n","\n","    # Create datasets using predefined function\n","    train_dataset = IncomeDataset(X_train_split, y_train_split)\n","    val_dataset = IncomeDataset(X_val, y_val)\n","    test_dataset = IncomeDataset(X_test, y_test)\n","\n","    # Create dataloaders using DataLoader function\n","    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True) # Not sure to shuffle or not\n","    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True) # Not sure to shuffle or not\n","\n","    input_size = X_train.shape[1] # Input size is the same as the shape of X_train\n","    model = IncomeClassifier(input_size) # Instantiate the model\n","\n","    # Define loss function and optimizer\n","    criterion = nn.BCELoss() # Claude-Sonnet 3.5 suggested BCE because of binary output\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.01) # Claude-Sonnet 3.5 implement Adam optimizer\n","      # No momentum because using Adam, for SGD include 'momentum=0.9'\n","\n","    # Train the model with early stopping\n","    train_losses, val_losses = train_model( # Call the train_model function to train using the arguments below\n","        model,\n","        train_loader,\n","        val_loader,\n","        criterion,\n","        optimizer,\n","        max_epochs=50, # Increased max epochs since we have early stopping\n","        patience=10, # Will stop if no improvement for 15 epochs\n","        min_delta=1e-5 # Minimum change in validation loss to qualify as an improvement\n","    )\n","\n","    # Evaluate final model\n","    model.eval() # SET TO EVAL MODE\n","    with torch.no_grad(): # No gradient because we're using test dataset\n","        correct = 0\n","        total = 0\n","        for inputs, labels in test_loader: # Reiterate using test dataloader data\n","            outputs = model(inputs)\n","            predicted = (outputs > 0.5).float() # Threshold for binary classification\n","            total += len(labels)\n","            correct += ((predicted.view(-1) == labels.view(-1))).sum().item() # Claude-Sonnet 3.5\n","            # This has to be written like this so the shapes/input matches the expected\n","\n","        accuracy = 100 * correct / total\n","        print(f'\\nFinal Test Accuracy: {accuracy:.2f}%')\n","\n","\n","\n","if __name__ == \"__main__\": # I don't understand why I need this but I do - Claude-Sonnet 3.5\n","    main() # Call main function to run everything"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q2mfvdF83Q8g","outputId":"53b8bffb-3a6a-4b49-91f0-cf98ad87d939","executionInfo":{"status":"ok","timestamp":1731098240573,"user_tz":360,"elapsed":72775,"user":{"displayName":"Jin Bai","userId":"02958359284509703486"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training Dataset Verification:\n","Feature shape: (32561, 81)\n","Label shape: (32561,)\n","Feature dtype: float32\n","Label dtype: float32\n","Any NaN in features: False\n","Any NaN in labels: False\n","Feature value range: [-3.530, 13.395]\n","Unique label values: [0. 1.]\n","\n","Test Dataset Verification:\n","Feature shape: (16281, 81)\n","Label shape: (16281,)\n","Feature dtype: float32\n","Label dtype: float32\n","Any NaN in features: False\n","Any NaN in labels: False\n","Feature value range: [-3.530, 13.395]\n","Unique label values: [0. 1.]\n","Epoch [1/50]: Train Loss: 0.3517, Train Acc: 83.29%, Val Loss: 0.3152, Val Acc: 85.29%\n","Epoch [2/50]: Train Loss: 0.3361, Train Acc: 84.31%, Val Loss: 0.3150, Val Acc: 85.37%\n","Epoch [3/50]: Train Loss: 0.3298, Train Acc: 84.51%, Val Loss: 0.3276, Val Acc: 84.83%\n","Epoch [4/50]: Train Loss: 0.3292, Train Acc: 84.69%, Val Loss: 0.3125, Val Acc: 85.81%\n","Epoch [5/50]: Train Loss: 0.3263, Train Acc: 85.02%, Val Loss: 0.3173, Val Acc: 85.58%\n","Epoch [6/50]: Train Loss: 0.3231, Train Acc: 84.98%, Val Loss: 0.3158, Val Acc: 85.37%\n","Epoch [7/50]: Train Loss: 0.3233, Train Acc: 85.01%, Val Loss: 0.3173, Val Acc: 85.35%\n","Epoch [8/50]: Train Loss: 0.3228, Train Acc: 84.95%, Val Loss: 0.3206, Val Acc: 85.61%\n","Epoch [9/50]: Train Loss: 0.3222, Train Acc: 85.19%, Val Loss: 0.3116, Val Acc: 85.83%\n","Epoch [10/50]: Train Loss: 0.3192, Train Acc: 85.27%, Val Loss: 0.3185, Val Acc: 85.54%\n","Epoch [11/50]: Train Loss: 0.3197, Train Acc: 84.94%, Val Loss: 0.3136, Val Acc: 85.29%\n","Epoch [12/50]: Train Loss: 0.3179, Train Acc: 85.30%, Val Loss: 0.3114, Val Acc: 85.83%\n","Epoch [13/50]: Train Loss: 0.3191, Train Acc: 85.16%, Val Loss: 0.3111, Val Acc: 86.10%\n","Epoch [14/50]: Train Loss: 0.3175, Train Acc: 85.32%, Val Loss: 0.3089, Val Acc: 85.81%\n","Epoch [15/50]: Train Loss: 0.3181, Train Acc: 85.31%, Val Loss: 0.3116, Val Acc: 85.64%\n","Epoch [16/50]: Train Loss: 0.3158, Train Acc: 85.18%, Val Loss: 0.3131, Val Acc: 85.49%\n","Epoch [17/50]: Train Loss: 0.3181, Train Acc: 85.24%, Val Loss: 0.3104, Val Acc: 85.86%\n","Epoch [18/50]: Train Loss: 0.3171, Train Acc: 85.48%, Val Loss: 0.3232, Val Acc: 84.81%\n","Epoch [19/50]: Train Loss: 0.3136, Train Acc: 85.44%, Val Loss: 0.3111, Val Acc: 85.97%\n","Epoch [20/50]: Train Loss: 0.3142, Train Acc: 85.56%, Val Loss: 0.3107, Val Acc: 85.67%\n","Epoch [21/50]: Train Loss: 0.3129, Train Acc: 85.58%, Val Loss: 0.3153, Val Acc: 85.34%\n","Epoch [22/50]: Train Loss: 0.3147, Train Acc: 85.60%, Val Loss: 0.3093, Val Acc: 85.38%\n","Epoch [23/50]: Train Loss: 0.3118, Train Acc: 85.64%, Val Loss: 0.3156, Val Acc: 85.23%\n","Epoch [24/50]: Train Loss: 0.3091, Train Acc: 85.80%, Val Loss: 0.3087, Val Acc: 85.75%\n","Epoch [25/50]: Train Loss: 0.3100, Train Acc: 85.62%, Val Loss: 0.3106, Val Acc: 85.46%\n","Epoch [26/50]: Train Loss: 0.3104, Train Acc: 85.65%, Val Loss: 0.3072, Val Acc: 85.35%\n","Epoch [27/50]: Train Loss: 0.3098, Train Acc: 85.70%, Val Loss: 0.3090, Val Acc: 85.58%\n","Epoch [28/50]: Train Loss: 0.3102, Train Acc: 85.57%, Val Loss: 0.3085, Val Acc: 85.61%\n","Epoch [29/50]: Train Loss: 0.3059, Train Acc: 85.84%, Val Loss: 0.3133, Val Acc: 85.32%\n","Epoch [30/50]: Train Loss: 0.3092, Train Acc: 85.91%, Val Loss: 0.3163, Val Acc: 85.34%\n","Epoch [31/50]: Train Loss: 0.3083, Train Acc: 85.84%, Val Loss: 0.3131, Val Acc: 85.55%\n","Epoch [32/50]: Train Loss: 0.3081, Train Acc: 85.71%, Val Loss: 0.3122, Val Acc: 85.48%\n","Epoch [33/50]: Train Loss: 0.3054, Train Acc: 85.72%, Val Loss: 0.3109, Val Acc: 85.44%\n","Epoch [34/50]: Train Loss: 0.3056, Train Acc: 85.58%, Val Loss: 0.3111, Val Acc: 85.17%\n","Epoch [35/50]: Train Loss: 0.3074, Train Acc: 85.81%, Val Loss: 0.3149, Val Acc: 85.28%\n","Epoch [36/50]: Train Loss: 0.3072, Train Acc: 85.75%, Val Loss: 0.3089, Val Acc: 85.74%\n","\n","Early stopping triggered after 36 epochs\n","\n","Final Test Accuracy: 85.59%\n"]}]}]}